{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型构造\n",
    "让我们回顾⼀下在[“多层感知机的简洁实现”](../dlbasic/3.10mlp-gluon.ipynb) ⼀节中含单隐藏层的多层感知机的实现⽅法。我们\n",
    "⾸先构造Sequential实例，然后依次添加两个全连接层。其中第⼀层的输出⼤小为256，即隐\n",
    "藏层单元个数是256；第⼆层的输出⼤小为10，即输出层单元个数是10。我们在上⼀章的其他节\n",
    "中也使⽤了Sequential类构造模型。这⾥我们介绍另外⼀种基于Block类的模型构造⽅法：它\n",
    "让模型构造更加灵活。\n",
    "## 继承Block类来构造模型\n",
    "Block类是nn模块⾥提供的⼀个模型构造类，我们可以继承它来定义我们想要的模型。下⾯继\n",
    "承Block类构造本节开头提到的多层感知机。这⾥定义的MLP类重载了Block类的__init__函\n",
    "数和forward函数。它们分别⽤于创建模型参数和定义前向计算。前向计算也即正向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from mxnet.gluon import nn\n",
    "class MLP(nn.Block):\n",
    "    # 声明带有模型参数的层，这⾥声明了两个全连接层\n",
    "    def __init__(self, **kwargs):\n",
    "        # 调⽤MLP⽗类Block的构造函数来进⾏必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "        # 参数，如“模型参数的访问、初始化和共享”⼀节将介绍的模型参数params\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(256, activation='relu')\n",
    "        self.output = nn.Dense(10)\n",
    "    # 定义模型的前向计算，即如何根据输⼊x计算返回所需要的模型输出\n",
    "    def forward(self, x):\n",
    "        return self.output(self.hidden(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "以上的MLP类中⽆须定义反向传播函数。系统将通过⾃动求梯度而⾃动⽣成反向传播所需\n",
    "的backward函数。我们可以实例化MLP类得到模型变量net。下⾯的代码初始化net并传⼊\n",
    "输⼊数据X做⼀次前向计算。其中， net(X)会调⽤MLP继承⾃Block类的__call__函数，\n",
    "这个函数将调⽤MLP类定义的forward函数来完成前向计算。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[-0.00884219 -0.01361075  0.00989831 -0.05785033 -0.09113112 -0.03276934\n   0.10951708 -0.0594931  -0.03217464 -0.03013538]\n [ 0.01740783  0.01566995 -0.12216819 -0.00195891 -0.00130794 -0.0514056\n   0.0844209   0.00769345  0.02565179 -0.02311425]]\n<NDArray 2x10 @cpu(0)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "X = nd.random.normal(shape=(2,20))\n",
    "net = MLP()\n",
    "net.initialize()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意，这⾥并没有将Block类命名为Layer（层）或者Model（模型）之类的名字，这是因为该\n",
    "类是⼀个可供⾃由组建的部件。它的⼦类既可以是⼀个层（如Gluon提供的Dense类），⼜可以是\n",
    "⼀个模型（如这⾥定义的MLP类），或者是模型的⼀个部分。我们下⾯通过两个例⼦来展⽰它的灵\n",
    "活性。\n",
    "## Sequential类继承自Block\n",
    "我们刚刚提到， Block类是⼀个通⽤的部件。事实上， Sequential类继承⾃Block类。当模型的前\n",
    "向计算为简单串联各个层的计算时，可以通过更加简单的⽅式定义模型。这正是Sequential类\n",
    "的⽬的：它提供add函数来逐⼀添加串联的Block⼦类实例，而模型的前向计算就是将这些实例\n",
    "按添加的顺序逐⼀计算。\n",
    "下⾯我们实现⼀个与Sequential类有相同功能的MySequential类。这或许可以帮助读者更\n",
    "加清晰地理解Sequential类的⼯作机制。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class MySequential(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MySequential, self).__init__(**kwargs)\n",
    "    def add(self, block):\n",
    "        # block是⼀个Block⼦类实例，假设它有⼀个独⼀⽆⼆的名字。我们将它保存在Block类的\n",
    "        # 成员变量_children⾥，其类型是OrderedDict。当MySequential实例调⽤\n",
    "        # initialize函数时，系统会⾃动对_children⾥所有成员初始化\n",
    "        self._children[block.name] = block\n",
    "    def forward(self, x):\n",
    "        # OrderedDict保证会按照成员添加时的顺序遍历成员\n",
    "        for block in self._children.values():\n",
    "            x = block(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们⽤MySequential类来实现前⾯描述的MLP类，并使⽤随机初始化的模型做⼀次前向计算。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[-0.05044801  0.08025599  0.04171951 -0.02426371  0.06428756 -0.00904521\n   0.08975367 -0.05393827  0.01967067  0.12805972]\n [-0.00394633  0.0259387   0.02448444 -0.18089381  0.23608395 -0.0582966\n  -0.01937858 -0.04208054  0.09123673 -0.05099846]]\n<NDArray 2x10 @cpu(0)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 13
    }
   ],
   "source": [
    "net = MySequential()\n",
    "net.add(nn.Dense(256,activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以观察到这⾥MySequential类的使⽤跟[“多层感知机的简洁实现”](../dlbasic/3.10mlp-gluon.ipynb) ⼀节中Sequential类的\n",
    "使⽤没什么区别。\n",
    "## 构造复杂的模型\n",
    "虽然Sequential类可以使模型构造更加简单，且不需要定义forward函数，但直接继承Block类\n",
    "可以极⼤地拓展模型构造的灵活性。下⾯我们构造⼀个稍微复杂点的⽹络FancyMLP。在这个⽹\n",
    "络中，我们通过get_constant函数创建训练中不被迭代的参数，即常数参数。在前向计算中，\n",
    "除了使⽤创建的常数参数外，我们还使⽤NDArray的函数和Python的控制流，并多次调⽤相同的层。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class FancyMLP(nn.Block):\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(FancyMLP, self).__init__(**kwargs)\n",
    "\t\t# 使用get_constant创建的随机权重参数不会在训练中被迭代（即常数参数）\n",
    "\t\tself.rand_weight = self.params.get_constant(\n",
    "\t\t\t'rand_weight', nd.random.uniform(shape=(20, 20)))\n",
    "\t\tself.dense = nn.Dense(20, activation='relu')\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.dense(x)\n",
    "\t\t# 使用创建的常数参数，以及NDArray的relu函数和dot函数\n",
    "\t\tx = nd.relu(nd.dot(x, self.rand_weight.data()) + 1)\n",
    "\t\t# 复用全连接层。等价于两个全连接层共享参数\n",
    "\t\tx = self.dense(x)\n",
    "\t\t# 控制流，这里我们需要调用asscalar函数来返回标量进行比较\n",
    "\t\twhile x.norm().asscalar() > 1:\n",
    "\t\t\tx /= 2\n",
    "\t\tif x.norm().asscalar() < 0.8:\n",
    "\t\t\tx *= 10\n",
    "\t\treturn x.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在这个FancyMLP模型中，我们使⽤了常数权重rand_weight（注意它不是模型参数）、做了矩\n",
    "阵乘法操作（nd.dot）并重复使⽤了相同的Dense层。下⾯我们来测试该模型的随机初始化和\n",
    "前向计算。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "\n[24.901281]\n<NDArray 1 @cpu(0)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 15
    }
   ],
   "source": [
    "net = FancyMLP()\n",
    "net.initialize()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "因为FancyMLP和Sequential类都是Block类的⼦类，所以我们可以嵌套调⽤它们"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "\n[4.0478725]\n<NDArray 1 @cpu(0)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 16
    }
   ],
   "source": [
    "class NestMLP(nn.Block):\n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(NestMLP, self).__init__(**kwargs)\n",
    "\t\tself.net = nn.Sequential()\n",
    "\t\tself.net.add(nn.Dense(64, activation='relu'),\n",
    "\t\t\t\t\t nn.Dense(32, activation='relu'))\n",
    "\t\tself.dense = nn.Dense(16, activation='relu')\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.dense(self.net(x))\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(NestMLP(), nn.Dense(20), FancyMLP())\n",
    "\n",
    "net.initialize()\n",
    "net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "- 可以通过继承Block类来构造模型。\n",
    "- Sequential类继承⾃Block类。\n",
    "- 虽然Sequential类可以使模型构造更加简单，但直接继承Block类可以极⼤地拓展模型构造的灵活性。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}