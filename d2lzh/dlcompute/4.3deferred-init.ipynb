{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型参数的延后初始化\n",
    "如果做了上⼀节练习，你会发现模型net在调⽤初始化函数initialize之后、在做前向计\n",
    "算net(X)之前时，权重参数的形状中出现了0。虽然直觉上initialize完成了所有参数初始\n",
    "化过程，然而这在Gluon中却是不⼀定的。我们在本节中详细讨论这个话题。\n",
    "## 延后初始化\n",
    "也许读者早就注意到了，在之前使⽤Gluon创建的全连接层都没有指定输⼊个数。例如，在上⼀节\n",
    "使⽤的多层感知机net⾥，我们创建的隐藏层仅仅指定了输出⼤小为256。当调⽤initialize函\n",
    "数时，由于隐藏层输⼊个数依然未知，系统也⽆法得知该层权重参数的形状。只有在当我们将\n",
    "形状是(2, 20)的输⼊X传进⽹络做前向计算net(X)时，系统才推断出该层的权重参数形状为(256,\n",
    "20)。因此，这时候我们才能真正开始初始化参数。\n",
    "\n",
    "让我们使⽤上⼀节中定义的MyInit类来演⽰这⼀过程。我们创建多层感知机，并使⽤MyInit实\n",
    "例来初始化模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from mxnet import nd, init\n",
    "from mxnet.gluon import nn\n",
    "class MyInit(init.Initializer):\n",
    "    def _init_weight(self, name, data):\n",
    "        print('Init', name, data.shape)\n",
    "        # 实际的初始化逻辑在此省略了\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'),\n",
    "        nn.Dense(10))\n",
    "net.initialize(init = MyInit())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "注意，虽然MyInit被调⽤时会打印模型参数的相关信息，但上⾯的initialize函数执⾏完并\n",
    "未打印任何信息。由此可⻅，调⽤initialize函数时并没有真正初始化参数。下⾯我们定义输\n",
    "⼊并执⾏⼀次前向计算"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "X = nd.random.uniform(shape=(2,20))\n",
    "y=net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "这时候，有关模型参数的信息被打印出来。在根据输⼊X做前向计算时，系统能够根据输⼊的形\n",
    "状⾃动推断出所有层的权重参数的形状。系统在创建这些参数之后，调⽤MyInit实例对它们进\n",
    "⾏初始化，然后才进⾏前向计算。\n",
    "当然，这个初始化只会在第⼀次前向计算时被调⽤。之后我们再运⾏前向计算net(X)时则不会\n",
    "重新初始化，因此不会再次产⽣MyInit实例的输出。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "y=net(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "系统将真正的参数初始化延后到获得⾜够信息时才执⾏的⾏为叫作延后初始化（deferred initialization）。\n",
    "它可以让模型的创建更加简单：只需要定义每个层的输出⼤小，而不⽤⼈⼯推测它们\n",
    "的输⼊个数。这对于之后将介绍的定义多达数⼗甚⾄数百层的⽹络来说尤其⽅便。\n",
    "然而，任何事物都有两⾯性。正如本节开头提到的那样，延后初始化也可能会带来⼀定的困惑。\n",
    "在第⼀次前向计算之前，我们⽆法直接操作模型参数，例如⽆法使⽤data函数和set_data函数\n",
    "来获取和修改参数。因此，我们经常会额外做⼀次前向计算来迫使参数被真正地初始化。\n",
    "## 避免延后初始化\n",
    "如果系统在调⽤initialize函数时能够知道所有参数的形状，那么延后初始化就不会发⽣。我\n",
    "们在这⾥分别介绍两种这样的情况。\n",
    "第⼀种情况是我们要对已初始化的模型重新初始化时。因为参数形状不会发⽣变化，所以系统能\n",
    "够⽴即进⾏重新初始化。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Init dense0_weight (256, 20)\n",
      "Init dense1_weight (10, 256)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "net.initialize(init=MyInit(), force_reinit=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "第⼆种情况是我们在创建层的时候指定了它的输⼊个数，使系统不需要额外的信息来推测参数形\n",
    "状。下例中我们通过in_units来指定每个全连接层的输⼊个数，使初始化能够在initialize函\n",
    "数被调⽤时⽴即发⽣。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Init dense2_weight (256, 20)\n",
      "Init dense3_weight (10, 256)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, in_units=20, activation='relu'))\n",
    "net.add(nn.Dense(10, in_units=256))\n",
    "net.initialize(init=MyInit())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 小结\n",
    "- 系统将真正的参数初始化延后到获得⾜够信息时才执⾏的⾏为叫作延后初始化。\n",
    "- 延后初始化的主要好处是让模型构造更加简单。例如，我们⽆须⼈⼯推测每个层的输⼊个数。\n",
    "- 也可以避免延后初始化。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}