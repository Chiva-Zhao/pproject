{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 多层感知机的从零开始实现\n",
    "我们已经从上⼀节⾥了解了多层感知机的原理。下⾯，我们⼀起来动⼿实现⼀个多层感知机。⾸\n",
    "先导⼊实现所需的包或模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mxnet import nd\n",
    "import d2lzh as d2l\n",
    "from mxnet.gluon import loss as gloss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 获取和读取数据\n",
    "这⾥继续使⽤Fashion-MNIST数据集。我们将使⽤多层感知机对图像进⾏分类。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_data, test_data = d2l.load_data_fashion_mnist(batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义模型参数\n",
    "我们在[“softmax回归的从零开始实现”](3.6softmax-regression-scratch.ipynb)⼀节⾥已经介绍了，Fashion-MNIST数据集中图像形状\n",
    "为28 × 28，类别数为10。本节中我们依然使⽤⻓度为28 × 28 = 784的向量表⽰每⼀张图像。因此，\n",
    "输⼊个数为784，输出个数为10。实验中，我们设超参数隐藏单元个数为256。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens = 784, 10, 256\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens))\n",
    "b1 = nd.zeros(num_hiddens)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens, num_outputs))\n",
    "b2 = nd.zeros(num_outputs)\n",
    "params = [W1, b1, W2, b2]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义激活函数\n",
    "这⾥我们使⽤基础的maximum函数来实现ReLU，而⾮直接调⽤relu函数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return nd.maximum(x,0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义模型\n",
    "同softmax回归⼀样，我们通过reshape函数将每张原始图像改成⻓度为num_inputs的向量。然\n",
    "后我们实现上⼀节中多层感知机的计算表达式。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    X =  X.reshape((-1, num_inputs))\n",
    "    H = relu(nd.dot(X, W1) + b1)\n",
    "    return nd.dot(H, W2) + b2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 定义损失函数\n",
    "为了得到更好的数值稳定性，我们直接使⽤Gluon提供的包括softmax运算和交叉熵损失计算的函数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "loss = gloss.SoftmaxCrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练模型\n",
    "训练多层感知机的步骤和[“softmax回归的从零开始实现”](3.6softmax-regression-scratch.ipynb)⼀节中训练softmax回归的步骤没什么\n",
    "区别。我们直接调⽤d2lzh包中的train_ch3函数，它的实现已经在[“softmax回归的从零开始\n",
    "实现”](3.6softmax-regression-scratch.ipynb)⼀节⾥介绍过。我们在这⾥设超参数迭代周期数为5，学习率为0.5。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "epoch 1, loss 0.8002, train acc 0.700, test acc 0.815\n",
      "epoch 2, loss 0.4922, train acc 0.818, test acc 0.838\n",
      "epoch 3, loss 0.4267, train acc 0.842, test acc 0.869\n",
      "epoch 4, loss 0.3948, train acc 0.854, test acc 0.868\n",
      "epoch 5, loss 0.3692, train acc 0.864, test acc 0.867\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "num_epochs, lr = 5, 0.5\n",
    "d2l.train_ch3(net,train_data,test_data,loss,num_epochs,batch_size,params,lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}