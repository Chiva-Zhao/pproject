{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 含并⾏连结的⽹络\n",
    "在2014年的ImageNet图像识别挑战赛中，⼀个名叫GoogLeNet的⽹络结构⼤放异彩 [1]。它虽然\n",
    "在名字上向LeNet致敬，但在⽹络结构上已经很难看到LeNet的影⼦。 GoogLeNet吸收了NiN中⽹\n",
    "络串联⽹络的思想，并在此基础上做了很⼤改进。在随后的⼏年⾥，研究⼈员对GoogLeNet进⾏\n",
    "了数次改进，本节将介绍这个模型系列的第⼀个版本。\n",
    "## Inception块\n",
    "GoogLeNet中的基础卷积块叫作Inception块，得名于同名电影《盗梦空间》（Inception）。与上\n",
    "⼀节介绍的NiN块相⽐，这个基础块在结构上更加复杂，如图5.8所⽰\n",
    "\n",
    "![Inception块的结构](../img/inception.svg)\n",
    "\n",
    "由图5.8可以看出， Inception块⾥有4条并⾏的线路。前3条线路使⽤窗口⼤小分别是1 × 1、 3 ×\n",
    "3和5 × 5的卷积层来抽取不同空间尺⼨下的信息，其中中间2个线路会对输⼊先做1 × 1卷积来减\n",
    "少输⼊通道数，以降低模型复杂度。第四条线路则使⽤3 × 3最⼤池化层，后接1 × 1卷积层来改变\n",
    "通道数。 4条线路都使⽤了合适的填充来使输⼊与输出的⾼和宽⼀致。最后我们将每条线路的输\n",
    "出在通道维上连结，并输⼊接下来的层中去。\n",
    "\n",
    "Inception块中可以⾃定义的超参数是每个层的输出通道数，我们以此来控制模型复杂度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import gluon, init, nd\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "class Inception(nn.Block):\n",
    "\t# c1 - c4为每条线路里的层的输出通道数\n",
    "\tdef __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "\t\tsuper(Inception, self).__init__(**kwargs)\n",
    "\t\t# 线路1，单1 x 1卷积层\n",
    "\t\tself.p1_1 = nn.Conv2D(c1, kernel_size=1, activation='relu')\n",
    "\t\t# 线路2，1 x 1卷积层后接3 x 3卷积层\n",
    "\t\tself.p2_1 = nn.Conv2D(c2[0], kernel_size=1, activation='relu')\n",
    "\t\tself.p2_2 = nn.Conv2D(c2[1], kernel_size=3, padding=1, activation='relu')\n",
    "\t\t# 线路3，1 x 1卷积层后接5 x 5卷积层\n",
    "\t\tself.p3_1 = nn.Conv2D(c3[0], kernel_size=1, activation='relu')\n",
    "\t\tself.p3_2 = nn.Conv2D(c3[1], kernel_size=5, padding=2, activation='relu')\n",
    "\t\t# 线路4，3 x 3最大池化层后接1 x 1卷积层\n",
    "\t\tself.p4_1 = nn.MaxPool2D(pool_size=3, strides=1, padding=1)\n",
    "\t\tself.p4_2 = nn.Conv2D(c4, kernel_size=1, activation='relu')\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tp1 = self.p1_1(x)\n",
    "\t\tp2 = self.p2_2(self.p2_1(x))\n",
    "\t\tp3 = self.p3_2(self.p3_1(x))\n",
    "\t\tp4 = self.p4_2(self.p4_1(x))\n",
    "\t\treturn nd.concat(p1, p2, p3, p4, dim=1)  # 在通道维上连结输出"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GoogLeNet模型\n",
    "GoogLeNet跟VGG⼀样，在主体卷积部分中使⽤5个模块（block），每个模块之间使⽤步幅为2的3×\n",
    "3最⼤池化层来减小输出⾼宽。第⼀模块使⽤⼀个64通道的7 × 7卷积层。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "b1 = nn.Sequential()\n",
    "b1.add(nn.Conv2D(64,kernel_size=7,strides=2,padding=3,activation='relu'),\n",
    "       nn.MaxPool2D(pool_size=3,strides=2,padding=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "第⼆模块使⽤2个卷积层：⾸先是64通道的1 × 1卷积层，然后是将通道增⼤3倍的3 × 3卷积层。它\n",
    "对应Inception块中的第⼆条线路。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "b2 = nn.Sequential()\n",
    "b2.add(nn.Conv2D(64, kernel_size=1, activation='relu'),\n",
    "       nn.Conv2D(192, kernel_size=3, padding=1, activation='relu'),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "第三模块串联2个完整的Inception块。第⼀个Inception块的输出通道数为64+128+32+32 = 256，\n",
    "其中4条线路的输出通道数⽐例为64 : 128 : 32 : 32 = 2 : 4 : 1 : 1。其中第⼆、第三条线路先分别将\n",
    "输⼊通道数减小⾄96/192 = 1/2和16/192 = 1/12后，再接上第⼆层卷积层。第⼆个Inception块\n",
    "输出通道数增⾄128 + 192 + 96 + 64 = 480，每条线路的输出通道数之⽐为128 : 192 : 96 : 64 =\n",
    "4 : 6 : 3 : 2。其中第⼆、第三条线路先分别将输⼊通道数减小⾄128/256 = 1/2和32/256 = 1/8。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "b3 = nn.Sequential()\n",
    "b3.add(Inception(64, (96, 128), (16, 32), 32),\n",
    "       Inception(128, (128, 192), (32, 96), 64),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "第四模块更加复杂。它串联了5个Inception块，其输出通道数分别是192 + 208 + 48 + 64 = 512、\n",
    "160+224+64+64 = 512、128+256+64+64 = 512、112+288+64+64 = 528和256+320+128+128 =\n",
    "832。这些线路的通道数分配和第三模块中的类似，⾸先含3 × 3卷积层的第⼆条线路输出最多通\n",
    "道，其次是仅含1×1卷积层的第⼀条线路，之后是含5×5卷积层的第三条线路和含3×3最⼤池化\n",
    "层的第四条线路。其中第⼆、第三条线路都会先按⽐例减小通道数。这些⽐例在各个Inception块\n",
    "中都略有不同。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "b4 = nn.Sequential()\n",
    "b4.add(Inception(192, (96, 208), (16, 48), 64),\n",
    "       Inception(160, (112, 224), (24, 64), 64),\n",
    "       Inception(128, (128, 256), (24, 64), 64),\n",
    "       Inception(112, (144, 288), (32, 64), 64),\n",
    "       Inception(256, (160, 320), (32, 128), 128),\n",
    "       nn.MaxPool2D(pool_size=3, strides=2, padding=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "第五模块有输出通道数为256 + 320 + 128 + 128 = 832和384 + 384 + 128 + 128 = 1024的两\n",
    "个Inception块。其中每条线路的通道数的分配思路和第三、第四模块中的⼀致，只是在具体数值\n",
    "上有所不同。需要注意的是，第五模块的后⾯紧跟输出层，该模块同NiN⼀样使⽤全局平均池化\n",
    "层来将每个通道的⾼和宽变成1。最后我们将输出变成⼆维数组后接上⼀个输出个数为标签类别\n",
    "数的全连接层。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "b5 = nn.Sequential()\n",
    "b5.add(Inception(256, (160, 320), (32, 128), 128), \n",
    "       Inception(384, (192, 384), (48, 128), 128),\n",
    "       nn.GlobalAvgPool2D())\n",
    "net = nn.Sequential()\n",
    "net.add(b1, b2, b3, b4, b5, nn.Dense(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "GoogLeNet模型的计算复杂，而且不如VGG那样便于修改通道数。本节⾥我们将输⼊的⾼和宽\n",
    "从224降到96来简化计算。下⾯演⽰各个模块之间的输出的形状变化。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "sequential0 output shape:\t (1, 64, 24, 24)\n",
      "sequential1 output shape:\t (1, 192, 12, 12)\n",
      "sequential2 output shape:\t (1, 480, 6, 6)\n",
      "sequential3 output shape:\t (1, 832, 3, 3)\n",
      "sequential4 output shape:\t (1, 1024, 1, 1)\n",
      "dense0 output shape:\t (1, 10)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "X = nd.random.uniform(shape=(1, 1, 96, 96))\n",
    "net.initialize()\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.name, 'output shape:\\t', X.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 获取数据和训练模型\n",
    "我们使⽤⾼和宽均为96像素的图像来训练GoogLeNet模型。训练使⽤的图像依然来⾃FashionMNIST数据集。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "lr, num_epochs, batch_size, ctx = 0.1, 5, 128, d2l.try_gpu()\n",
    "net.initialize(force_reinit=True, ctx=ctx, init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n",
    "d2l.train_ch5(net, train_iter, test_iter, batch_size, trainer, ctx,num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ⼩结\n",
    "- Inception块相当于⼀个有4条线路的⼦⽹络。它通过不同窗口形状的卷积层和最⼤池化层\n",
    "来并⾏抽取信息，并使⽤1 × 1卷积层减少通道数从而降低模型复杂度。\n",
    "- GoogLeNet将多个设计精细的Inception块和其他层串联起来。其中Inception块的通道数分\n",
    "配之⽐是在ImageNet数据集上通过⼤量的实验得来的。\n",
    "- GoogLeNet和它的后继者们⼀度是ImageNet上最⾼效的模型之⼀：在类似的测试精度下，\n",
    "它们的计算复杂度往往更低。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}