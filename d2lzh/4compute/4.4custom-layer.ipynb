{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 自定义层\n",
    "深度学习的⼀个魅⼒在于神经⽹络中各式各样的层，例如全连接层和后⾯章节中将要介绍的卷积\n",
    "层、池化层与循环层。虽然Gluon提供了⼤量常⽤的层，但有时候我们依然希望⾃定义层。本节\n",
    "将介绍如何使⽤NDArray来⾃定义⼀个Gluon的层，从而可以被重复调⽤。\n",
    "## 不含模型参数的⾃定义层\n",
    "我们先介绍如何定义⼀个不含模型参数的⾃定义层。事实上，这和“模型构造” ⼀节中介绍的使\n",
    "⽤Block类构造模型类似。下⾯的CenteredLayer类通过继承Block类⾃定义了⼀个将输⼊减\n",
    "掉均值后输出的层，并将层的计算定义在了forward函数⾥。这个层⾥不含模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from mxnet import gluon, nd\n",
    "from mxnet.gluon import nn\n",
    "class CenteredLayer(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CenteredLayer,self).__init__(**kwargs)\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们可以实例化这个层，然后做前向计算。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "\n[-2.5 -1.5 -0.5  0.5  1.5  2.5]\n<NDArray 6 @cpu(0)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "net = CenteredLayer()\n",
    "net(nd.array([1,2,3,4,5,6]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们也可以⽤它来构造更复杂的模型。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(128),CenteredLayer())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下⾯打印⾃定义层各个输出的均值。因为均值是浮点数，所以它的值是⼀个很接近0的数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "-7.212293e-10"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "net.initialize()\n",
    "y = net(nd.random.uniform(shape=(4,8)))\n",
    "y.mean().asscalar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 含模型参数的⾃定义层\n",
    "我们还可以⾃定义含模型参数的⾃定义层。其中的模型参数可以通过训练学出。\n",
    "[“模型参数的访问、初始化和共享”](4.2parameters.ipynb) ⼀节分别介绍了Parameter类和ParameterDict类。在⾃定\n",
    "义含模型参数的层时，我们可以利⽤Block类⾃带的ParameterDict类型的成员变量params。\n",
    "它是⼀个由字符串类型的参数名字映射到Parameter类型的模型参数的字典。我们可以通\n",
    "过get函数从ParameterDict创建Parameter实例。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(\n  Parameter params (shape=(2, 3), dtype=<class 'numpy.float32'>)\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "params = gluon.ParameterDict()\n",
    "params.get('params',shape=(2,3))\n",
    "params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "现在我们尝试实现⼀个含权重参数和偏差参数的全连接层。它使⽤ReLU函数作为激活函数。其\n",
    "中in_units和units分别代表输⼊个数和输出个数。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class MyDense(nn.Block):\n",
    "    # units为该层的输出个数， in_units为该层的输⼊个数\n",
    "    def __init__(self, units, in_units, **kwargs):\n",
    "        super(MyDense,self).__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=(in_units,units))\n",
    "        self.bias = self.params.get('bias',shape=(units,))\n",
    "    def forward(self, x):\n",
    "        linear = nd.dot(x, self.weight.data() + self.bias.data())\n",
    "        return nd.relu(linear)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面我们实例化MyDense类并访问它的模型参数。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "mydense0_ (\n  Parameter mydense0_weight (shape=(5, 3), dtype=<class 'numpy.float32'>)\n  Parameter mydense0_bias (shape=(3,), dtype=<class 'numpy.float32'>)\n)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "dense = MyDense(units=3, in_units=5)\n",
    "dense.params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们可以直接使⽤⾃定义层做前向计算。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[0.12833305 0.03455773 0.02007208]\n [0.08881921 0.06478509 0.        ]]\n<NDArray 2x3 @cpu(0)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "dense.initialize()\n",
    "dense(nd.random.uniform(shape=(2,5)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们也可以使⽤⾃定义层构造模型。它和Gluon的其他层在使⽤上很类似。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[0.03783609]\n [0.03518325]]\n<NDArray 2x1 @cpu(0)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 11
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(MyDense(8,64),\n",
    "        MyDense(1,8))\n",
    "net.initialize()\n",
    "net(nd.random.uniform(shape=(2,64)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "- 可以通过Block类⾃定义神经⽹络中的层，从而可以被重复调⽤。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}