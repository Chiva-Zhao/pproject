{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 线性回归\n",
    "线性回归输出是⼀个连续值，因此适⽤于回归问题。回归问题在实际中很常⻅，如预测房屋价\n",
    "格、⽓温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是⼀个离散值。\n",
    "我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。\n",
    "softmax回归则适⽤于分类问题。\n",
    "由于线性回归和softmax回归都是单层神经⽹络，它们涉及的概念和技术同样适⽤于⼤多数的深\n",
    "度学习模型。我们⾸先以线性回归为例，介绍⼤多数深度学习模型的基本要素和表⽰⽅法\n",
    "## 线性回归的基本要素\n",
    "### 模型\n",
    "设房屋的⾯积为$x1$，房龄为$x2$，售出价格为$y$。我们需要建⽴基于输⼊$x1$和$x2$来计算输出$y$的表达\n",
    "式，也就是模型（model）。顾名思义，线性回归假设输出与各个输⼊之间是线性关系：\n",
    "\n",
    "$$\\hat y = x_1w_1 + x_2w_2 + b;$$\n",
    "\n",
    "其中$w1$和$w2$是权重（weight）， $b$是偏差（bias），且均为标量。它们是线性回归模型的参数（parameter）。\n",
    "模型输出$\\hat y$是线性回归对真实价格$y$的预测或估计。我们通常允许它们之间有⼀定误\n",
    "差。\n",
    "### 模型训练\n",
    "接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程\n",
    "叫作模型训练（model training）。下⾯我们介绍模型训练所涉及的3个要素。\n",
    "#### 1. 训练数据\n",
    "假设我们采集的样本数为$n$，索引为$i$的样本的特征为$x_1^{(i)}$和$x_2^{(i)}$，标签为$y^{(i)}$。\n",
    "对于索引为$i$的房屋，线性回归模型的房屋价格预测表达式为\n",
    "\n",
    "$$\\hat y ^{(i)} = x_1^iw_1 + x_2^iw_2 + b$$\n",
    "\n",
    "#### 2. 损失函数\n",
    "在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取⼀个⾮负数作为\n",
    "误差，且数值越小表⽰误差越小。⼀个常⽤的选择是平⽅函数。它在评估索引为$i$的样本误差的表\n",
    "达式为\n",
    "\n",
    "$$ℓ(i)(w1; w2; b) = \\frac12(\\hat y^{(i)} − y^{(i)})^2$$\n",
    "\n",
    "其中常数1/2使对平⽅项求导后的常数系数为1，这样在形式上稍微简单⼀些。显然，误差越小表\n",
    "⽰预测价格与真实价格越相近，且当⼆者相等时误差为0。给定训练数据集，这个误差只与模型\n",
    "参数相关，因此我们将它记为以模型参数为参数的函数。在机器学习⾥，将衡量误差的函数称为\n",
    "损失函数（loss function）。这⾥使⽤的平⽅误差函数也称为平⽅损失（square loss）。\n",
    "通常，我们⽤训练数据集中所有样本误差的平均来衡量模型预测的质量，即\n",
    "$ℓ(w1; w2; b) = \\frac1n\\sum_{i=1}^n ℓ(i)(w1; w2; b) = \\frac1n\\sum_{i=1}^n \\frac12(x_1^{(i)}w_1 +x_2^{(i)}w_2 + b − y^{(i)})^2 $\n",
    "\n",
    "在模型训练中，我们希望找出⼀组模型参数，记为$w_1^∗; w_2^∗; b^∗$，来使训练样本平均损失最小：\n",
    "$$w_1^∗; w_2^∗; b^∗ = argmin[ℓ(w1; w2; b)]$$\n",
    "#### 3. 优化算法\n",
    "当模型和损失函数形式较为简单时，上⾯的误差最小化问题的解可以直接⽤公式表达出来。这类\n",
    "解叫作解析解（analytical solution）。本节使⽤的线性回归和平⽅误差刚好属于这个范畴。然而，\n",
    "⼤多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函\n",
    "数的值。这类解叫作数值解（numerical solution）。\n",
    "\n",
    "在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深\n",
    "度学习中被⼴泛使⽤。它的算法很简单：先选取⼀组模型参数的初始值，如随机选取；接下来对\n",
    "参数进⾏多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样⼀个\n",
    "由固定数⽬训练数据样本所组成的小批量（mini-batch） $\\mathcal{B}$，然后求小批量中数据样本的平均损\n",
    "失有关模型参数的导数（梯度），最后⽤此结果与预先设定的⼀个正数的乘积作为模型参数在本\n",
    "次迭代的减小量。\n",
    "在训练本节讨论的线性回归模型的过程中，模型的每个参数将作如下迭代：\n",
    "\n",
    "$\n",
    "    \\begin{aligned}\n",
    "    w_1 &\\leftarrow w_1 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b)  }{\\partial w_1} = w_1 -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_1^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\\\\n",
    "    w_2 &\\leftarrow w_2 - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b)  }{\\partial w_2} = w_2 -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}x_2^{(i)} \\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right),\\\\\n",
    "    b &\\leftarrow b - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\frac{ \\partial \\ell^{(i)}(w_1, w_2, b)  }{\\partial b} = b -   \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}}\\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\\right).\n",
    "    \\end{aligned},\n",
    "$\n",
    "\n",
    "在上式中， $\\mathcal{B}$代表每个小批量中的样本个数（批量⼤小， batch size）， $\\eta$称作学习率（learning\n",
    "rate）并取正数。需要强调的是，这⾥的批量⼤小和学习率的值是⼈为设定的，并不是通过模型\n",
    "训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参\n",
    "数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。\n",
    "### 模型预测\n",
    "模型训练完成后，我们将模型参数$w_1; w_2; b$在优化算法停⽌时的值分别记作$\\hat w_1; \\hat w_2;\\hat b$。注意，这⾥\n",
    "我们得到的并不⼀定是最小化损失函数的最优解$w_1^∗; w_2^∗; b^∗$，而是对最优解的⼀个近似。然后，我\n",
    "们就可以使⽤学出的线性回归模型$x_1\\hat w_1 + x2\\hat w_2 + \\hat b$来估算训练数据集以外任意⼀栋⾯积（平⽅\n",
    "⽶）为$x_1$、房龄（年）为$x_2$的房屋的价格了。这⾥的估算也叫作模型预测、模型推断或模型测试\n",
    "## 线性回归的表⽰⽅法\n",
    "线性回归是⼀个单层神经⽹络。输出层中负责计算o的\n",
    "单元⼜叫神经元。在线性回归中， o的计算依赖于$x_1$和$x_2$。也就是说，输出层中的神经元和输⼊\n",
    "层中各个输⼊完全连接。因此，这⾥的输出层⼜叫全连接层（fully-connected layer）或稠密层\n",
    "（dense layer）\n",
    "### ⽮量计算表达式\n",
    "向量相加的⼀种⽅法是，将这两个向量按元素逐⼀做标量加法\n",
    "向量相加的另⼀种⽅法是，将这两个向量直接做⽮量加法。\n",
    "结果很明显，后者⽐前者更省时。因此，我们应该尽可能采⽤⽮量计算，以提升计算效率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from mxnet import nd\n",
    "from time import time\n",
    "a = nd.ones(shape=1000)\n",
    "b = nd.ones(shape=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0.19248437881469727"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "start = time()\n",
    "c = nd.zeros(shape=1000)\n",
    "for i in range(1000):\n",
    "    c[i] = a[i] + b[i]\n",
    "time() - start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 5
    }
   ],
   "source": [
    "start = time()\n",
    "d = a + b\n",
    "time() - start"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "让我们再次回到本节的房价预测问题。如果我们对训练数据集⾥的3个房屋样本（索引分别为1、\n",
    "2和3）逐⼀预测价格，将得到\n",
    "\n",
    "$\n",
    "    \\begin{aligned}\n",
    "    \\hat y^{1} &= x_1^{(1)}w_1 + x_2^{(1)}w_2 + b,\\\\\n",
    "    \\hat y^{2} &= x_1^{(2)}w_1 + x_2^{(2)}w_2 + b,\\\\\n",
    "    \\hat y^{3} &= x_1^{(3)}w_1 + x_2^{(3)}w_2 + b,\n",
    "    \\end{aligned}\n",
    "$\n",
    "\n",
    "现在，我们将上⾯3个等式转化成⽮量计算。设\n",
    "\n",
    "$\n",
    "    \\boldsymbol {\\hat{y}} =\\\\\n",
    "    \\begin{bmatrix}\n",
    "        \\hat{y}^{(1)} \\\\\n",
    "        \\hat{y}^{(2)} \\\\\n",
    "        \\hat{y}^{(3)}\n",
    "    \\end{bmatrix},\\quad\n",
    "    \\boldsymbol{X} =\n",
    "    \\begin{bmatrix}\n",
    "        x_1^{(1)} & x_2^{(1)} \\\\\n",
    "        x_1^{(2)} & x_2^{(2)} \\\\\n",
    "        x_1^{(3)} & x_2^{(3)}\n",
    "    \\end{bmatrix},\\quad\n",
    "    \\boldsymbol{w} =\n",
    "    \\begin{bmatrix}\n",
    "        w_1 \\\\\n",
    "        w_2,\n",
    "    \\end{bmatrix}.\n",
    "$\n",
    "\n",
    "对3个房屋样本预测价格的⽮量计算表达式为$\\hat y = Xw + b$; 其中的加法运算使⽤了⼴播机制\n",
    "⼴义上讲，当数据样本数为n，特征数为d时，线性回归的⽮量计算表达式为\n",
    "$\\hat y = Xw + b$\n",
    "其中模型输出$\\hat y \\in \\mathbb R^{n×1}$，批量数据样本特征$X \\in \\mathbb R^{n×d}$，\n",
    "权重$w \\in \\mathbb R^{d×1}$，偏差$b \\in \\mathbb R$。相应地，批量数据样本标签$y \\in \\mathbb R^{n×1}$。\n",
    "设模型参数$θ = [w1; w2; b]^⊤$，我们可以重写损失函数为\n",
    "\n",
    "$\\ell(\\theta) = \\frac{1}{2n}(\\hat y -y)^T(\\hat y - y)$\n",
    "\n",
    "小批量随机梯度下降的迭代步骤将相应地改写为\n",
    "\n",
    "$\\ell(\\theta) = \\theta - \\frac{\\eta}{\\lvert\\mathcal B\\rvert} \\sum_{i \\in \\mathcal{B}}\\nabla_\\theta\\ell^{(i)}(\\theta)$\n",
    "\n",
    "其中梯度是损失有关的3个为标量的模型参数的偏导数组成的向量\n",
    "\n",
    "$\\nabla_\\theta\\ell^{(i)}(\\theta) = \\\n",
    "\\begin{bmatrix}\\\n",
    "\\frac{ \\partial{\\ell^{(i)}(w_1,w_2,b)}} {\\partial{w_1}}\\\\\n",
    "\\frac{ \\partial{\\ell^{(i)}(w_1,w_2,b)}} {\\partial{w_2}}\\\\\n",
    "\\frac{ \\partial{\\ell^{(i)}(w_1,w_2,b)}} {\\partial{b}}\\\n",
    "\\end{bmatrix}\\\n",
    "=\n",
    "\\begin{bmatrix}\\\n",
    "x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y_{(i)})\\\\\n",
    "x_2^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y_{(i)})\\\\\n",
    "x_1^{(i)}w_1+x_2^{(i)}w_2+b-y_{(i)}\\\\\n",
    "\\end{bmatrix}\\\n",
    "=\n",
    "\\begin{bmatrix}\\\n",
    "x_1^{(i)}\\\\\n",
    "x_2^{(i)}\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix}\\\n",
    "(\\hat y^{(i)}-y^{(i)})\n",
    "$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}