{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# softmax回归\n",
    "前⼏节介绍的线性回归模型适⽤于输出为连续值的情景。在另⼀类情景中，模型输出可以是⼀个\n",
    "像图像类别这样的离散值。对于这样的离散值预测问题，我们可以使⽤诸如softmax回归在内的\n",
    "分类模型。和线性回归不同， softmax回归的输出单元从⼀个变成了多个，且引⼊了softmax运算\n",
    "使输出更适合离散值的预测和训练。本节以softmax回归模型为例，介绍神经⽹络中的分类模型\n",
    "## 分类问题\n",
    "让我们考虑⼀个简单的图像分类问题，其输⼊图像的⾼和宽均为2像素，且⾊彩为灰度。这样每\n",
    "个像素值都可以⽤⼀个标量表⽰。我们将图像中的4像素分别记为$x1; x2; x3; x4$。假设训练数据集\n",
    "中图像的真实标签为狗、猫或鸡（假设可以⽤4像素表⽰出这3种动物），这些标签分别对应离散\n",
    "值$y1; y2; y3$。我们通常使⽤离散的数值来表⽰类别，例如$y1 = 1; y2 = 2; y3 = 3$。如此，⼀张图像的标签为1、\n",
    "2和3这3个数值中的⼀个。虽然我们仍然可以使⽤回归模型来进⾏建模，并将预测值就近定点化\n",
    "到1、 2和3这3个离散值之⼀，但这种连续值到离散值的转化通常会影响到分类质量。因此我们⼀\n",
    "般使⽤更加适合离散值输出的模型来解决分类问题\n",
    "## softmax回归模型\n",
    "softmax回归跟线性回归⼀样将输⼊特征与权重做线性叠加。与线性回归的⼀个主要不同在于，\n",
    "softmax回归的输出值个数等于标签⾥的类别数。因为⼀共有4种特征和3种输出动物类别，所以\n",
    "权重包含12个标量（带下标的$w$）、偏差包含3个标量（带下标的$b$），且对每个输⼊计算\n",
    "$o1; o2; o3$这3个输出：\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "o_1 &= x_1w_{11} + x_2w_{21} + x_3w_{31} + x_4w_{41} + b1\\\\\n",
    "o_2 &= x_1w_{12} + x_2w_{22} + x_3w_{32} + x_4w_{42} + b2\\\\\n",
    "o_3 &= x_1w_{13} + x_2w_{23} + x_3w_{33} + x_4w_{43} + b3\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "softmax回归同线性回归⼀样，也是⼀个单层神经⽹络。\n",
    "由于每个输出$o1; o2; o3$的计算都要依赖于所有的输⼊$x1; x2; x3; x4$， \n",
    "softmax回归的输出层也是⼀个全连接层\n",
    "### softmax运算\n",
    "既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值$o_i$当作预测类别是$i$的置信度，\n",
    "并将值最大的输出所对应的类作为预测输出，即输出$\\operatorname*{argmax}_i o_i$。例如，\n",
    "如果$o_1,o_2,o_3$分别为$0.1,10,0.1$，由于$o_2$最大，那么预测类别为2，其代表猫。\n",
    "然而，直接使⽤输出层的输出有两个问题。⼀⽅⾯，由于输出层的输出值的范围不确定，我们难\n",
    "以直观上判断这些值的意义。例如，刚才举的例⼦中的输出值10表⽰“很置信”图像类别为猫，\n",
    "因为该输出值是其他两类的输出值的100倍。但如果o1 = o3 = 103，那么输出值10却⼜表⽰图像\n",
    "类别为猫的概率很低。另⼀⽅⾯，由于真实标签是离散值，这些离散值与不确定范围的输出值之\n",
    "间的误差难以衡量。softmax运算符（softmax operator）解决了以上两个问题。\n",
    "它通过下式将输出值变换成值为正且和为1的概率分布\n",
    "\n",
    "$\\hat y_1,\\hat y_2,\\hat y_3 = \\text softmax(o_1,o_2,o_3)$， 其中\n",
    "\n",
    "$\\hat y_1=\\frac{exp(o_1)}{\\sum_{i=0}^3exp(o_i)}, \\quad \\hat y_2=\\frac{exp(o_2)}{\\sum_{i=0}^3exp(o_i)}, \\quad\\hat y_3=\\frac{exp(o_3)}{\\sum_{i=0}^3exp(o_i)}$\n",
    "\n",
    "其中$\\hat y_1+\\hat y_2+\\hat y_3 = 1且0\\leq \\hat y_1,\\hat y_2,\\hat y_3\\leq1$,因此\n",
    "$\\hat y_1,\\hat y_2,\\hat y_3$是一个合法的概率分布。这时候，如果$\\hat y_2 = 0.8$，\n",
    "不管$y_1和y_3$的值是多少，我们都知道图像类别为猫的概率是80%。此外，我们注意到\n",
    "\n",
    "$$\\operatorname*{argmax}_i o_i = \\operatorname*{argmax}_i \\hat y_i,$$\n",
    "因此softmax运算不改变预测类别输出\n",
    "## 单样本分类的⽮量计算表达式\n",
    "为了提⾼计算效率，我们可以将单样本分类通过⽮量计算来表达。在上⾯的图像分类问题中，假\n",
    "设softmax回归的权重和偏差参数分别为\n",
    "\n",
    "$\n",
    "\\boldsymbol {W} = \n",
    "\\begin{bmatrix}\n",
    "w_{11}\\ & w_{12} & w_{13} \\\\\n",
    "w_{21}\\ & w_{22} & w_{23} \\\\\n",
    "w_{31}\\ & w_{32} & w_{33} \\\n",
    "\\end{bmatrix}\n",
    ", \\boldsymbol b=\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & b_3\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "设⾼和宽分别为2个像素的图像样本i的特征为\n",
    "$x^i = \n",
    "\\begin{bmatrix}\n",
    "x_1^{(i)} & x_2^{(i)} & x_3^{(i)} & x_4^{(i)}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "输出层的输出为：$o^i = \n",
    "\\begin{bmatrix}\n",
    "o_1^{(i)} & o_2^{(i)} & o_3^{(i)}\n",
    "\\end{bmatrix}\n",
    "$预测为狗、猫或鸡的概率分布为\n",
    "$\\hat y^i = \n",
    "\\begin{bmatrix}\n",
    "\\hat y_1^{(i)} & \\hat y_2^{(i)} & \\hat y_3^{(i)}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "softmax回归对样本i分类的⽮量计算表达式为\n",
    "$\n",
    "    \\begin{aligned}\n",
    "    \\boldsymbol{o}^{(i)} &= \\boldsymbol{x}^{(i)} \\boldsymbol{W} + \\boldsymbol{b}\\\\\n",
    "    \\boldsymbol{\\hat{y}}^{(i)} &= \\text{softmax}(\\boldsymbol{o}^{(i)})\n",
    "    \\end{aligned}\n",
    "$\n",
    "## ⼩批量样本分类的⽮量计算表达式\n",
    "为了进⼀步提升计算效率，我们通常对小批量数据做⽮量计算。⼴义上讲，给定⼀个小批量样本，\n",
    "其批量⼤小为$n$，输⼊个数（特征数）为$d$，输出个数（类别数）为$q$。设批量特征为$\\boldsymbol{X} \\in \\mathbb{R}^{n×d}$。\n",
    "假设softmax回归的权重和偏差参数分别为$\\boldsymbol{W} \\in \\mathbb{R}^{d×q}和\\boldsymbol{b} \\in \\mathbb{R}^{1×q}$。 \n",
    "softmax回归的⽮量计算表达式为\n",
    "\n",
    "$\\begin{aligned}\n",
    "\\boldsymbol{O} &= \\boldsymbol{X} \\boldsymbol{W} + \\boldsymbol{b},\\\\\n",
    "\\boldsymbol{\\hat Y} &= \\text softmax(\\boldsymbol{O})\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "其中的加法运算使⽤了⼴播机制， $O,Y \\in \\mathbb{R}^{n×q}$且这两个矩阵的第$i$⾏\n",
    "分别为样本$i的输出o^{(i)}和概率分布y^{(i)}$。\n",
    "## 交叉熵损失函数\n",
    "前⾯提到，使⽤softmax运算后可以更⽅便地与离散标签计算误差。我们已经知道， softmax运算\n",
    "将输出变换成⼀个合法的类别预测分布。实际上，真实标签也可以⽤类别分布表达：对于样本$i$，\n",
    "我们构造向量$y^{(i)} \\in \\mathbb{R}^q$ ，使其第$y^{(i)}$（样本$i$类别的离散数值）个元素为1，\n",
    "其余为0。这样我们的训练⽬标可以设为使预测概率分布$\\hat y^{(i)}$尽可能接近真实的标签概率分布$y^{(i)}$。\n",
    "我们可以像线性回归那样使⽤平⽅损失函数$\\lVert \\hat y^{(i)} − y^{(i)}\\rVert ^2/2$。然而，想要预测分类结果正确，我\n",
    "们其实并不需要预测概率完全等于标签概率。例如，在图像分类的例⼦⾥，如果$y^{(i)} = 3$，那么\n",
    "我们只需要$\\hat y_3^{(i)}$⽐其他两个预测值$\\hat y_1^{(i)}$和$\\hat y_2^{(i)}$⼤就⾏了。\n",
    "即使$\\hat y_3^{(i)}$值为0.6，不管其他两个预测值为多少，类别预测均正确。而平⽅损失则过于严格，\n",
    "例如$\\hat y_1^{(i)} = \\hat y_2^{(i)}= 0.2$⽐$\\hat y_1^{(i)} = 0,\\hat y_2^{(i)}= 0.4$\n",
    "的损失要小很多，虽然两者都有同样正确的分类预测结果\n",
    "改善上述问题的⼀个⽅法是使⽤更适合衡量两个概率分布差异的测量函数。其中，交叉熵（cross\n",
    "entropy）是⼀个常⽤的衡量⽅法：\n",
    "$$H\\left(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}\\right ) = -\\sum_{j=1}^q y_j^{(i)} \\log \\hat y_j^{(i)},$$\n",
    "\n",
    "其中带下标的$y_j^{(i)}$是向量$\\boldsymbol y^{(i)}$中⾮0即1的元素，需要注意将它与样本i类别的离散数值，即不\n",
    "带下标的$y^{(i)}$区分。在上式中，我们知道向量$\\boldsymbol y^{(i)}$中只有第$y^{(i)}$个元素\n",
    "$y^{(i)}_{y^{(i)}}$为1，其余全为0，于是$H(\\boldsymbol y^{(i)}, \\boldsymbol {\\hat y}^{(i)}) = -\\log \\hat y_{y^{(i)}}^{(i)}$\n",
    "。也就是说，交叉熵只关⼼对正确类别的预测概率，因为只要其值⾜够⼤，就可以确保分类结果正确。\n",
    "当然，遇到⼀个样本有多个标签时，例如图像⾥含有不⽌⼀个物体时，我们并不能做这⼀步简化。\n",
    "但即便对于这种情况，交叉熵同样只关⼼对图像中出现的物体类别的预测概率。\n",
    "假设训练数据集的样本数为$n$，交叉熵损失函数定义为\n",
    "\n",
    "$$\\ell (\\Theta) = \\frac {1}{n}\\sum_{i=1}^nH \\left(y_{(i)},\\hat y_{(i)}\\right)$$\n",
    "\n",
    "其中$\\Theta$代表模型参数。同样地，如果每个样本只有⼀个标签，那么交叉熵损失可以简写成\n",
    "$\\ell (\\Theta) = -\\frac {1}{n}\\sum_{i=1}^n \\log \\hat y_{y^{(i)}}^{(i)}$。\n",
    "从另⼀个⻆度来看，我们知道最小化ℓ(Θ)等价于最⼤化$exp(−nℓ(Θ)) =\\Pi_{i=1}^n\\log \\hat y_{y^{(i)}}^{(i)}$,\n",
    "即最小化交叉熵损失函数等价于最⼤化训练数据集所有标签类别的联合预测概率\n",
    "## 模型预测及评价\n",
    "在训练好softmax回归模型后，给定任⼀样本特征，就可以预测每个输出类别的概率。通常，我\n",
    "们把预测概率最⼤的类别作为输出类别。如果它与真实类别（标签）⼀致，说明这次预测是正确\n",
    "的。在之后[“softmax回归的从零开始实现”](softmax-regression-scratch.ipynb)⼀节的实验中，\n",
    "我们将使⽤准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之⽐\n",
    "\n",
    "- softmax回归适⽤于分类问题。它使⽤softmax运算输出类别的概率分布。\n",
    "- softmax回归是⼀个单层神经⽹络，输出个数等于分类问题中的类别个数。\n",
    "- 交叉熵适合衡量两个概率分布的差异"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}