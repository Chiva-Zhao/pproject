{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 丢弃法\n",
    "除了前⼀节介绍的权重衰减以外，深度学习模型常常使⽤丢弃法（dropout） [1] 来应对过拟合问\n",
    "题。丢弃法有⼀些不同的变体。本节中提到的丢弃法特指倒置丢弃法（inverted dropout）\n",
    "## 方法\n",
    "回忆⼀下，[ “多层感知机”](3.8mlp.ipynb) ⼀节的图描述了⼀个单隐藏层的多层感知机。其中输⼊个数为4，隐\n",
    "藏单元个数为5，且隐藏单元$h_i(i = 1,...,5)$的计算表达式为\n",
    "\n",
    "$$h_i=\\phi(x_1w_{1i}+ x_2w_{2i}+x_3w_{3i}+x_4w_{4i}+b_i)$$\n",
    "\n",
    "这⾥$\\phi$是激活函数， $x_1,\\ldots, x_4$是输⼊，隐藏单元i的权重参数为$w_{1i},\\ldots,w_{4i}$，偏差参数为$b_i$。\n",
    "当对该隐藏层使⽤丢弃法时，该层的隐藏单元将有⼀定概率被丢弃掉。设丢弃概率为p，那么有p的概\n",
    "率$h_i$会被清零，有1 − p的概率$h_i$会除以1 − p做拉伸。丢弃概率是丢弃法的超参数。具体来说，设\n",
    "随机变量$\\xi_i$为0和1的概率分别为p和1 − p。使⽤丢弃法时我们计算新的隐藏单元$h'_i$\n",
    "$$h'_i = \\frac{\\xi_i}{1-p}h $$\n",
    "\n",
    "由于$E(\\xi_i) = 1 − p$，因此$$E(h'_i) = \\frac{E(\\xi_i)}{1-p}h = h_i$$\n",
    "即丢弃法不改变其输⼊的期望值。让我们对图中的隐藏层使⽤丢弃法，⼀种可能的结果如\n",
    "图所⽰，其中h2和h5被清零。这时输出值的计算不再依赖h2和h5，在反向传播时，与这两个隐\n",
    "藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即$h_1,\\ldots,h_5$都有\n",
    "可能被清零，输出层的计算⽆法过度依赖$h_1,\\ldots,h_5$中的任⼀个，从而在训练模型时起到正则化的\n",
    "作⽤，并可以⽤来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，⼀般不使⽤丢\n",
    "弃法。\n",
    "\n",
    "![隐藏层使用了丢弃法的多层感知机](../img/dropout.svg)\n",
    "\n",
    "## 从零开始实现\n",
    "根据丢弃法的定义，我们可以很容易地实现它。下⾯的dropout函数将以drop_prob的概率丢\n",
    "弃NDArray输⼊X中的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import d2lzh as d2l\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "\n",
    "def dropout(X, drop_prob):\n",
    "    assert 0 <= drop_prob <= 1\n",
    "    keep_prob = 1 - drop_prob\n",
    "    # 这种情况下把全部元素都丢弃\n",
    "    if keep_prob == 0:\n",
    "        return X.zeros_like()\n",
    "    mask = nd.random.uniform(0, 1, X.shape) < keep_prob\n",
    "    return mask * X / keep_prob"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们运⾏⼏个例⼦来测试⼀下dropout函数。其中丢弃概率分别为0、 0.5和1。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[ 0.  1.  2.  3.  4.  5.  6.  7.]\n [ 8.  9. 10. 11. 12. 13. 14. 15.]]\n<NDArray 2x8 @cpu(0)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "X = nd.arange(16).reshape((2, 8))\n",
    "dropout(X, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[ 0.  2.  4.  6.  0.  0.  0. 14.]\n [ 0. 18.  0.  0. 24. 26. 28.  0.]]\n<NDArray 2x8 @cpu(0)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "dropout(X, 0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "\n[[0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0.]]\n<NDArray 2x8 @cpu(0)>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "dropout(X, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义模型参数\n",
    "实验中，我们依然使⽤[“softmax回归的从零开始实现”](3.6softmax-regression-scratch.ipynb) ⼀节中介绍的Fashion-MNIST数据集。我\n",
    "们将定义⼀个包含两个隐藏层的多层感知机，其中两个隐藏层的输出个数都是256。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n",
    "\n",
    "W1 = nd.random.normal(scale=0.01, shape=(num_inputs, num_hiddens1))\n",
    "b1 = nd.zeros(num_hiddens1)\n",
    "W2 = nd.random.normal(scale=0.01, shape=(num_hiddens1, num_hiddens2))\n",
    "b2 = nd.zeros(num_hiddens2)\n",
    "W3 = nd.random.normal(scale=0.01, shape=(num_hiddens2, num_outputs))\n",
    "b3 = nd.zeros(num_outputs)\n",
    "params = [W1, b1, W2, b2, W3, b3]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义模型\n",
    "下⾯定义的模型将全连接层和激活函数ReLU串起来，并对每个激活函数的输出使⽤丢弃法。我\n",
    "们可以分别设置各个层的丢弃概率。通常的建议是把靠近输⼊层的丢弃概率设得小⼀点。在这个\n",
    "实验中，我们把第⼀个隐藏层的丢弃概率设为0.2，把第⼆个隐藏层的丢弃概率设为0.5。我们可\n",
    "以通过[“⾃动求梯度”]() ⼀节中介绍的is_training函数来判断运⾏模式为训练还是测试，并只\n",
    "需在训练模式下使⽤丢弃法。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "drop_prob1, drop_prob2 = 0.2, 0.5\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H1 = (nd.dot(X,W1) + b1).relu()\n",
    "    if autograd.is_training():  # 只在训练模型时使⽤丢弃法\n",
    "        H1 = dropout(H1, drop_prob1) # 在第⼀层全连接后添加丢弃层\n",
    "    H2 = (nd.dot(H1,W2) + b2).relu()\n",
    "    if autograd.is_training():\n",
    "        H2 = dropout(H2, drop_prob2)\n",
    "    return nd.dot(H2, W3) + b3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 训练和测试模型\n",
    "这部分与之前多层感知机的训练和测试类似。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "epoch 1, loss 1.0880, train acc 0.575, test acc 0.786\n",
      "epoch 2, loss 0.5717, train acc 0.785, test acc 0.837\n",
      "epoch 3, loss 0.4868, train acc 0.822, test acc 0.846\n",
      "epoch 4, loss 0.4447, train acc 0.837, test acc 0.847\n",
      "epoch 5, loss 0.4144, train acc 0.849, test acc 0.871\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "batch_size, num_epochs, lr = 256, 5, 0.5\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train_ch3(net, train_iter, test_iter,loss, num_epochs,batch_size,params, lr)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 简洁实现\n",
    "在Gluon中，我们只需要在全连接层后添加Dropout层并指定丢弃概率。在训练模型时，\n",
    "Dropout层将以指定的丢弃概率随机丢弃上⼀层的输出元素；在测试模型时， Dropout层并不\n",
    "发挥作⽤。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(256, activation='relu'),\n",
    "        nn.Dropout(drop_prob1), # 在第⼀个全连接层后添加丢弃层\n",
    "        nn.Dense(256, activation='relu'),\n",
    "        nn.Dropout(drop_prob2),\n",
    "        nn.Dense(10))\n",
    "net.initialize(init.Normal(sigma=0.01))# 在第二个全连接层后添加丢弃层"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下⾯训练并测试模型。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "epoch 1, loss 1.0956, train acc 0.578, test acc 0.779\n",
      "epoch 2, loss 0.5775, train acc 0.787, test acc 0.835\n",
      "epoch 3, loss 0.4895, train acc 0.821, test acc 0.853\n",
      "epoch 4, loss 0.4423, train acc 0.839, test acc 0.861\n",
      "epoch 5, loss 0.4145, train acc 0.848, test acc 0.858\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(),'sgd',{'learning_rate': lr})\n",
    "d2l.train_ch3(net,train_iter,test_iter,loss ,num_epochs,\n",
    "              batch_size, None,None,trainer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 小结\n",
    "- 我们可以通过使⽤丢弃法应对过拟合。\n",
    "- 丢弃法只在训练模型时使⽤。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}