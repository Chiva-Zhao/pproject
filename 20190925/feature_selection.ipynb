{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Feature selection strategies can be divided into three main areas based on the type of strategy and\n",
    "techniques employed for the same. They are described briefly as follows.\n",
    "- **Filter methods**: These techniques select features purely based on metrics like\n",
    "correlation, mutual information and so on. These methods do not depend on results\n",
    "obtained from any model and usually check the relationship of each feature with\n",
    "the response variable to be predicted. Popular methods include threshold based\n",
    "methods and statistical tests.\n",
    "- **Wrapper methods**: These techniques try to capture interaction between multiple\n",
    "features by using a recursive approach to build multiple models using feature\n",
    "subsets and select the best subset of features giving us the best performing model.\n",
    "Methods like backward selecting and forward elimination are popular wrapper\n",
    "based methods.\n",
    "- **Embedded methods**: These techniques try to combine the benefits of the other\n",
    "two methods by leveraging Machine Learning models themselves to rank and score\n",
    "feature variables based on their importance. Tree based methods like decision trees\n",
    "and ensemble methods like random forests are popular examples of embedded\n",
    "methods."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.set_printoptions(suppress=True)\n",
    "pt = np.get_printoptions()['threshold']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Threshold-Based Methods\n",
    "#### Limiting features in bag of word based models\n",
    "The scikit-learn framework provides parameters like min_df and max_\n",
    "df which can be used to specify thresholds for ignoring terms which have document frequency above and\n",
    "below user specified thresholds"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(min_df=0.1,max_df=.85,max_features=2000)\n",
    "cv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Variance based thresholding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/Pokemon.csv')\n",
    "poke_gen = pd.get_dummies(df['Generation'])\n",
    "poke_gen.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "vt = VarianceThreshold(.15)\n",
    "vt.fit(poke_gen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To view the variances as well as which features were finally selected by this algorithm, we can use the\n",
    "variances_ property and the get_support(...) function respectively. The following snippet depicts this\n",
    "clearly in a formatted dataframe"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame({'variance': vt.variances_, 'select_feature': vt.get_support()},\n",
    "             index=poke_gen.columns).T"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can clearly see which features have been selected based on their True values and also their variance\n",
    "being above 0.15. To get the final subset of selected features, you can use the following code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "poke_gen_subset = poke_gen.iloc[:,vt.get_support()].head()\n",
    "poke_gen_subset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Statistical Methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "bc_data = load_breast_cancer()\n",
    "bc_features = pd.DataFrame(bc_data.data, columns=bc_data.feature_names)\n",
    "bc_classes = pd.DataFrame(bc_data.target, columns=['IsMalignant'])\n",
    "# build featureset and response class labels\n",
    "bc_X = np.array(bc_features)\n",
    "bc_y = np.array(bc_classes).T[0]\n",
    "print('Feature set shape:', bc_X.shape)\n",
    "print('Response class shape:', bc_y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=30)\n",
    "print('Feature set data [shape: '+str(bc_X.shape)+']')\n",
    "print(np.round(bc_X, 2), '\\n')\n",
    "print('Feature names:')\n",
    "print(np.array(bc_features.columns), '\\n')\n",
    "print('Predictor Class label data [shape: '+str(bc_y.shape)+']')\n",
    "print(bc_y, '\\n')\n",
    "print('Predictor name:', np.array(bc_classes.columns))\n",
    "np.set_printoptions(threshold=pt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s now use the chi-square test on this feature set and select the top 15 best features\n",
    "out of the 30 features. The following snippet helps us achieve this"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "skb = SelectKBest(score_func=chi2, k=15)\n",
    "skb.fit(bc_X,bc_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_scores = [(item, score) for item, score in zip(bc_data.feature_names, skb.scores_)]\n",
    "sorted(feature_scores, key=lambda x: -x[1])[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now create a subset of the 15 selected features obtained from our original feature set of 30\n",
    "features with the help of the chi-square test by using the following code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "select_features_kbest = skb.get_support()\n",
    "feature_names_kbest = bc_data.feature_names[select_features_kbest]\n",
    "feature_subset_df = bc_features[feature_names_kbest]\n",
    "bc_SX = np.array(feature_subset_df)\n",
    "print(bc_SX.shape)\n",
    "print(feature_names_kbest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To view the new feature set, you can use the following snippet.\n",
    "feature_subset_df.iloc[20:25]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let’s now build a simple classification model using logistic regression on the original feature set of 30 features \n",
    "and compare the model accuracy performance with another model built using our selected 15 features. For model evaluation,\n",
    "we will use the accuracy metric (percent of correct predictions) and use a five-fold cross-validation scheme.\n",
    "The main idea here is to compare the model prediction performance between models trained on different feature sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "lr = LogisticRegression()\n",
    "# evaluating accuracy for model built on full featureset\n",
    "full_feat_acc = np.average(cross_val_score(lr, bc_X, bc_y, scoring='accuracy', cv=5))\n",
    "# evaluating accuracy for model built on selected featureset\n",
    "sel_feat_acc = np.average(cross_val_score(lr, bc_SX, bc_y, scoring='accuracy', cv=5))\n",
    "print('Model accuracy statistics with 5-fold cross validation')\n",
    "print('Model accuracy with complete feature set', bc_X.shape, ':', full_feat_acc)\n",
    "print('Model accuracy with selected feature set', bc_SX.shape, ':', sel_feat_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Recursive Feature Elimination\n",
    "Recursive Feature Elimination, also known as RFE, is a popular wrapper based feature selection technique,\n",
    "which allows you to use this strategy. The basic idea is to start off with a specific Machine Learning estimator\n",
    "like the Logistic Regression algorithm we used for our classification needs. Next we take the entire feature set\n",
    "of 30 features and the corresponding response class variables. RFE aims to assign weights to these features\n",
    "based on the model fit. Features with the smallest weights are pruned out and then a model is fit again on\n",
    "the remaining features to obtain the new weights or scores. This process is recursively carried out multiple\n",
    "times and each time features with the lowest scores/weights are eliminated, until the pruned feature subset\n",
    "contains the desired number of features that the user wanted to select (this is taken as an input parameter at\n",
    "the start). This strategy is also popularly known as backward elimination. Let’s select the top 15 features on\n",
    "our breast cancer dataset now using RFE."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "lr = LogisticRegression()\n",
    "rfe = RFE(estimator=lr, n_features_to_select=15, step=1)\n",
    "rfe.fit(bc_X, bc_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We can now use the get_support(...) function to obtain the final 15 selected features\n",
    "select_features_rfe = rfe.get_support()\n",
    "feature_names_rfe = bc_data.feature_names[select_features_rfe]\n",
    "print(feature_names_rfe)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "set(feature_names_kbest) & set(feature_names_rfe)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model based selection\n",
    "Tree based models like decision trees and ensemble models like random forests (ensemble of trees) can be utilized not \n",
    "just for modeling alone but for feature selection. These models can be used to compute feature importances when building\n",
    "the model that can in turn be used for selecting the best features and discarding irrelevant features with lower scores.\n",
    "Random forest is an ensemble model. This can be used as an embedded feature selection method, where each decision tree \n",
    "model in the ensemble is built by taking a training sample of data from the entire dataset. This sample is a bootstrap \n",
    "sample (sample taken with replacement). Splits at any node are taken by choosing the best split from a random subset of\n",
    "the features rather than taking all the features into account. This randomness tends to reduce the variance of the model\n",
    "at the cost of slightly increasing the bias. Overall this produces a better and more generalized model. \n",
    "Let’s now use the random forest model to score and rank features based on their importance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(bc_X, bc_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The following code uses this random forest estimator to score the features based on their importance\n",
    "# and we display the top 10 most important features based on this score.\n",
    "importance_scores = rfc.feature_importances_\n",
    "feature_importances = [(feature, score) for feature, score in zip(bc_data.feature_names, importance_scores)]\n",
    "sorted(feature_importances, key=lambda x: -x[1])[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Feature extraction using dimensionality reduction\n",
    "#### Feature Extraction with Principal Component Analysis\n",
    "Principal component analysis, popularly known as PCA, is a statistical method that uses the process of\n",
    "linear, orthogonal transformation to transform a higher-dimensional set of features that could be possibly\n",
    "correlated into a lower-dimensional set of linearly uncorrelated features. These transformed and newly\n",
    "created features are also known as Principal Components or PCs. In any PCA transformation, the total\n",
    "number of PCs is always less than or equal to the initial number of features. The first principal component\n",
    "tries to capture the maximum variance of the original set of features. Each of the succeeding components\n",
    "tries to capture more of the variance such that they are orthogonal to the preceding components. An\n",
    "important point to remember is that PCA is sensitive to feature scaling.\n",
    "Our main task is to take a set of initial features with dimension let’s say D and reduce it to a subset of\n",
    "extracted principal components of a lower dimension LD. The matrix decomposition process of Singular\n",
    "Value Decomposition is extremely useful in helping us obtain the principal components. \n",
    "Considering we have a data matrix $F_{n\\times d} $, where we have n observations and\n",
    "D dimensions (features), we can depict SVD of the feature matrix as $F_{n \\times D} = USV^T$ such that all the principal\n",
    "components are contained in the component V<sup>T</sup>, which can be depicted as follows:\n",
    "\n",
    "${V^T}_{(D\\times D)}=$\n",
    "$\\left[\n",
    "    \\begin{matrix}\n",
    "        PC_{1(1\\times D)} \\\\ \n",
    "        PC_{2(1\\times D)} \\\\ \n",
    "        \\vdots \\\\ \n",
    "        PC_{D(1\\times D)} \\\\ \n",
    "    \\end{matrix}\n",
    "\\right]\n",
    "$\n",
    "\n",
    "The principal components are represented by ${PC_1, PC_2, \\cdots, PC_D}$ , which are all one-dimensional vectors\n",
    "of dimensions (1 x D). For extracting the first d principal components, we can first transpose this matrix to\n",
    "obtain the following representation.\n",
    "\n",
    "$\n",
    "PC_{(D\\times D)}=(V^T)^T=[PC_{1(D\\times 1)}|PC_{2(D\\times 1)}|\\cdots|PC_{D(D\\times 1)}]\n",
    "$\n",
    "\n",
    "Let’s try to extract the first three principal components now from our breast cancer feature set of\n",
    "30 features using SVD. We first center our feature matrix and then use SVD and subsetting to extract the first\n",
    "three PCs using the following code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "(30, 3)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 59
    }
   ],
   "source": [
    "# center the feature set\n",
    "bc_XC = bc_X - bc_X.mean(axis=0)\n",
    "# decompose using SVD\n",
    "U, S, VT = np.linalg.svd(bc_XC)\n",
    "# get principal components\n",
    "PC = VT.T\n",
    "# get first 3 principal components\n",
    "PC3 = PC[:, 0:3]\n",
    "PC3.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-1160.14,  -293.92,   -48.58],\n       [-1269.12,    15.63,    35.39],\n       [ -995.79,    39.16,     1.71],\n       ...,\n       [ -314.5 ,    47.55,    10.44],\n       [-1124.86,    34.13,    19.74],\n       [  771.53,   -88.64,   -23.89]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 60
    }
   ],
   "source": [
    "# reduce feature set dimensionality \n",
    "np.round(bc_XC.dot(PC3), 2)\n",
    "# Thus you can see how powerful SVD and PCA can be in helping us reduce dimensionality by extracting\n",
    "# necessary features. Of course in Machine Learning systems and pipelines you can use utilities from scikitlearn \n",
    "# instead of writing unnecessary code and equations. The following code enables us to perform PCA on\n",
    "# our breast cancer feature set leveraging scikit-learn's APIs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "PCA(copy=True, iterated_power='auto', n_components=3, random_state=None,\n    svd_solver='auto', tol=0.0, whiten=False)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 61
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(bc_X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.98204467, 0.01617649, 0.00155751])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 63
    }
   ],
   "source": [
    "# To understand how much of the variance is explained by each of these principal components, you can use the following code.\n",
    "pca.explained_variance_ratio_\n",
    "# From the preceding output, as expected, we can see the maximum variance is explained by the first\n",
    "# principal component. To obtain the reduced feature set, we can use the following snippet."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1160.14, -293.92,   48.58],\n       [1269.12,   15.63,  -35.39],\n       [ 995.79,   39.16,   -1.71],\n       ...,\n       [ 314.5 ,   47.55,  -10.44],\n       [1124.86,   34.13,  -19.74],\n       [-771.53,  -88.64,   23.89]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 64
    }
   ],
   "source": [
    "bc_pca = pca.transform(bc_X)\n",
    "np.round(bc_pca, 2)\n",
    "# Let’s now quickly build a logistic regression model as before and use model accuracy and five-fold cross\n",
    "# validation to evaluate the model quality using these three features."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "0.9280800307810695"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 65
    }
   ],
   "source": [
    "np.average(cross_val_score(lr, bc_pca, bc_y, scoring='accuracy', cv=5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}